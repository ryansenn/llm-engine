# llm-engine

A native C++ inference engine that runs LLMs directly from model weights

## Todo
- [x] model binary converter
- [x] model in-memory loader
- [x] tokenizer
- [x] tensor + math ops
- [ ] transformer forward pass
    - [x] embeddings
    - [ ] attention
    - [ ] mlp
    - [ ] output
- [ ] fix segfaults
- [ ] cli input/output
- [ ] optimize speed
    - [ ] parallelization
    - [ ] quantization
    - [ ] CUDA kernel?  

